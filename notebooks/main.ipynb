{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffddce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d3ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt=\"\"\"\n",
    "Please act as an impartial judge and evaluate the quality of the response provided by two AI assistants to the input prompt. The responses should reflect\n",
    "knowledge of KNOWLEDGE SOURCE demonstrating specific and knowledgeable\n",
    "insights from CONTEXT about the query. Avoid positional Biasness. Just declare\n",
    "which response is better and provide one statement why. User's Query: {prompt}\n",
    "Assistant A Response: {base_model_response} \n",
    "Assistant B Response: {finetuned_model_response} \n",
    "\n",
    "You should choose the assistant that produces a better generation. Avoid positional biases and ensure that the order in which the\n",
    "responses were presented does not influence your decision. Be as objective as possible. After providing your explanation, output your final verdict strictly following\n",
    "this format: [[A]] if assistant A is better, [[B]] if assistant B is better, and [[C]] for a tie.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce4ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970548dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "# Use thinking_level for Gemini 3 models\n",
    "resp = completion(\n",
    "    model=\"gemini/gemini-3-pro-preview\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem step by step.\"}],\n",
    "    reasoning_effort=\"high\",  # Options: \"low\" or \"high\"\n",
    ")\n",
    "\n",
    "# Low thinking level for faster, simpler tasks\n",
    "resp = completion(\n",
    "    model=\"gemini/gemini-3-pro-preview\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the weather today?\"}],\n",
    "    reasoning_effort=\"low\",  # Minimizes latency and cost\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cac274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.3-70B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"YOUR_HF_TOKEN\", # HF Token for gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa580b5",
   "metadata": {},
   "source": [
    "## Connect to Google Colab with T4 GPU\n",
    "\n",
    "This section sets up a connection to Google Colab with T4 GPU access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package if not already installed\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"pyngrok\",\n",
    "    \"google-colab\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"✓ {package} installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {package} failed: {e}\")\n",
    "\n",
    "print(\"\\nPackages installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setup ngrok authentication\n",
    "from pyngrok import ngrok\n",
    "import getpass\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NGROK SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Create a free account at https://ngrok.com\")\n",
    "print(\"2. Get your authentication token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "print(\"\\nPaste your ngrok auth token below:\")\n",
    "\n",
    "ngrok_auth_token = getpass.getpass(\"ngrok auth token: \")\n",
    "\n",
    "if ngrok_auth_token:\n",
    "    ngrok.set_auth_token(ngrok_auth_token)\n",
    "    print(\"✓ ngrok authentication configured\")\n",
    "else:\n",
    "    print(\"✗ ngrok token not provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU AVAILABILITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"\\nGPU Available: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\n⚠ No GPU detected. Please run this in Google Colab with GPU enabled:\")\n",
    "    print(\"  - Go to Runtime → Change runtime type → Hardware accelerator → GPU (T4)\")\n",
    "    print(\"  - This script is designed to be executed in Colab with T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Start Jupyter server and create ngrok tunnel\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COLAB JUPYTER CONNECTION SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get Jupyter token (if running in Colab, generate one)\n",
    "JUPYTER_TOKEN = \"colab_session_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"\\nJupyter Token: {JUPYTER_TOKEN}\")\n",
    "print(\"\\nStarting ngrok tunnel...\")\n",
    "\n",
    "try:\n",
    "    # Create tunnel to localhost:8888 (default Jupyter port)\n",
    "    public_url = ngrok.connect(8888, \"http\")\n",
    "    print(f\"✓ Tunnel created successfully!\")\n",
    "    print(f\"\\nPublic URL: {public_url}\")\n",
    "    \n",
    "    # Extract ngrok URL\n",
    "    tunnel_url = public_url.public_url\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"CONNECTION DETAILS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nAccess URL: {tunnel_url}/?token={JUPYTER_TOKEN}\")\n",
    "    print(f\"\\nUse this link to connect to your Colab notebook from your local machine\")\n",
    "    print(f\"Replace 'localhost:8888' with this public URL in your local browser\")\n",
    "    \n",
    "    # Save connection info for reference\n",
    "    connection_info = {\n",
    "        \"public_url\": tunnel_url,\n",
    "        \"token\": JUPYTER_TOKEN,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"full_url\": f\"{tunnel_url}/?token={JUPYTER_TOKEN}\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n\" + json.dumps(connection_info, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating tunnel: {e}\")\n",
    "    print(\"Make sure you've configured ngrok auth token in the previous cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2124b",
   "metadata": {},
   "source": [
    "## How to Run This Script\n",
    "\n",
    "### Prerequisites\n",
    "1. **Google Colab Account** - Free GPU access at https://colab.research.google.com\n",
    "2. **ngrok Account** - Free tunnel service at https://ngrok.com\n",
    "3. **ngrok Auth Token** - Get it from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "### Steps\n",
    "1. Copy this code to a Google Colab notebook\n",
    "2. Run cells in order: Install → ngrok Auth → GPU Check → Connect\n",
    "3. Once tunnel is created, you'll get a public URL\n",
    "4. Open that URL in your browser to access Jupyter with T4 GPU\n",
    "\n",
    "### Alternative: Direct Colab Approach\n",
    "If you just want to use Colab's T4 GPU directly:\n",
    "- Go to https://colab.research.google.com\n",
    "- New notebook → Runtime → Change runtime type → GPU (T4)\n",
    "- Upload your notebook or write code directly\n",
    "- No tunnel needed!\n",
    "\n",
    "### Notes\n",
    "- Keep the tunnel running while using the connection\n",
    "- T4 GPU has 16GB VRAM (good for most LLMs)\n",
    "- Colab sessions timeout after 12 hours of inactivity\n",
    "- Free tier has usage limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Utility: Test Connection Status\n",
    "import socketserver\n",
    "import threading\n",
    "\n",
    "def test_gpu_and_connection():\n",
    "    \"\"\"Test GPU access and connection status\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONNECTION STATUS CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check GPU\n",
    "    try:\n",
    "        import torch\n",
    "        gpu_status = \"✓ GPU Available\" if torch.cuda.is_available() else \"✗ No GPU\"\n",
    "        print(f\"\\nGPU Status: {gpu_status}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU Check Error: {e}\")\n",
    "    \n",
    "    # Check ngrok\n",
    "    try:\n",
    "        from pyngrok import ngrok\n",
    "        tunnels = ngrok.get_tunnels()\n",
    "        print(f\"\\nActive Tunnels: {len(tunnels)}\")\n",
    "        for tunnel in tunnels:\n",
    "            print(f\"  - {tunnel.public_url} → {tunnel.config.addr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tunnel Check: No active tunnel (run Step 4 first)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Run the test\n",
    "test_gpu_and_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
