{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec78310",
   "metadata": {},
   "source": [
    "# Dataset Pipeline Skeleton (KnowSLM-style)\n",
    "\n",
    "This notebook gives a **skeleton pipeline** for:\n",
    "- loading Delhi-food and poetry source documents,\n",
    "- parsing raw files into clean text,\n",
    "- chunking for RAG/indexing,\n",
    "- generating synthetic conversation pairs (DSPy-style 2-step prompting),\n",
    "- exporting training-ready JSONL.\n",
    "\n",
    "You can fill in model-specific choices and run end-to-end incrementally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a484480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (run once)\n",
    "# %pip install -q pypdf beautifulsoup4 trafilatura litellm python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cee4a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\Pranav\\Desktop\\proj\\github_projects\\knowsLM_implementation\n",
      "DELHI_DIR exists: True\n",
      "POETRY_DIR exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATASET_ROOT = PROJECT_ROOT / 'data' / 'dataset'\n",
    "DELHI_DIR = DATASET_ROOT / 'delhi-food'\n",
    "POETRY_DIR = DATASET_ROOT / 'poetry'\n",
    "WORK_DIR = DATASET_ROOT / '_processed'\n",
    "\n",
    "PARSED_DIR = WORK_DIR / 'parsed_text'\n",
    "CHUNKS_DIR = WORK_DIR / 'chunks'\n",
    "EXPORT_DIR = WORK_DIR / 'exports'\n",
    "\n",
    "for d in [PARSED_DIR, CHUNKS_DIR, EXPORT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('DELHI_DIR exists:', DELHI_DIR.exists())\n",
    "print('POETRY_DIR exists:', POETRY_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ced9430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi files: 21\n",
      "Poetry files: 8\n",
      "Sample Delhi files:\n",
      "- 01_delhi-tourism_small-walk_brochure.pdf\n",
      "- 02_delhi-tourism_heritage-walk_itinerary.pdf\n",
      "- 03_holidify_delhi-street-food-guide.pdf\n",
      "- 04_holidify_cafes-in-delhi.pdf\n",
      "- 05_holidify_delhi_seasonal-travel-food-context.pdf\n"
     ]
    }
   ],
   "source": [
    "# File inventory\n",
    "\n",
    "def list_files(base: Path, exts=None):\n",
    "    exts = exts or ['.pdf', '.html', '.htm', '.txt', '.md']\n",
    "    files = [p for p in base.rglob('*') if p.is_file() and p.suffix.lower() in exts]\n",
    "    return sorted(files)\n",
    "\n",
    "delhi_files = list_files(DELHI_DIR)\n",
    "poetry_files = list_files(POETRY_DIR)\n",
    "\n",
    "print(f'Delhi files: {len(delhi_files)}')\n",
    "print(f'Poetry files: {len(poetry_files)}')\n",
    "print('Sample Delhi files:')\n",
    "for p in delhi_files[:5]:\n",
    "    print('-', p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d20fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing utilities (loss-minimizing skeleton)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pypdf import PdfReader\n",
    "\n",
    "try:\n",
    "    import trafilatura\n",
    "except Exception:\n",
    "    trafilatura = None\n",
    "\n",
    "\n",
    "def read_pdf(path: Path) -> str:\n",
    "    reader = PdfReader(str(path))\n",
    "    pages = []\n",
    "    for i, page in enumerate(reader.pages, start=1):\n",
    "        txt = page.extract_text() or ''\n",
    "        pages.append(f\"\\n\\n[PAGE {i}]\\n\" + txt)\n",
    "    return ''.join(pages).strip()\n",
    "\n",
    "\n",
    "def read_html(path: Path) -> str:\n",
    "    raw = path.read_text(encoding='utf-8', errors='ignore')\n",
    "    if trafilatura:\n",
    "        extracted = trafilatura.extract(raw, include_comments=False, include_tables=True)\n",
    "        if extracted:\n",
    "            return extracted.strip()\n",
    "\n",
    "    soup = BeautifulSoup(raw, 'html.parser')\n",
    "    for tag in soup(['script', 'style', 'noscript']):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator='')\n",
    "    text = re.sub(r'{3,}', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    return path.read_text(encoding='utf-8', errors='ignore').strip()\n",
    "\n",
    "\n",
    "def parse_document(path: Path) -> Dict[str, Any]:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == '.pdf':\n",
    "        text = read_pdf(path)\n",
    "    elif ext in {'.html', '.htm'}:\n",
    "        text = read_html(path)\n",
    "    elif ext in {'.txt', '.md'}:\n",
    "        text = read_text_file(path)\n",
    "    else:\n",
    "        text = ''\n",
    "\n",
    "    return {\n",
    "        'source_path': str(path),\n",
    "        'source_name': path.name,\n",
    "        'ext': ext,\n",
    "        'char_count': len(text),\n",
    "        'text': text,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65810959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and persist plain text snapshots\n",
    "\n",
    "def save_parsed(doc: Dict[str, Any], out_dir: Path):\n",
    "    stem = Path(doc['source_name']).stem\n",
    "    out_path = out_dir / f\"{stem}.txt\"\n",
    "    payload = doc['text']\n",
    "    out_path.write_text(payload, encoding='utf-8')\n",
    "    return out_path\n",
    "\n",
    "all_docs = []\n",
    "for p in delhi_files + poetry_files:\n",
    "    parsed = parse_document(p)\n",
    "    all_docs.append(parsed)\n",
    "    save_parsed(parsed, PARSED_DIR)\n",
    "\n",
    "print('Parsed docs:', len(all_docs))\n",
    "short_docs = [d for d in all_docs if d['char_count'] < 800]\n",
    "print('Potentially weak extractions (<800 chars):', len(short_docs))\n",
    "for d in short_docs[:10]:\n",
    "    print('-', d['source_name'], d['char_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking utilities (heading-agnostic baseline; replace with semantic splitter later)\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    source_name: str\n",
    "    source_path: str\n",
    "    corpus: str  # delhi-food / poetry\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "    text: str\n",
    "\n",
    "\n",
    "def simple_char_chunk(text: str, size: int = 1800, overlap: int = 300) -> List[Dict[str, Any]]:\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        j = min(i + size, n)\n",
    "        chunks.append({'start': i, 'end': j, 'text': text[i:j]})\n",
    "        if j == n:\n",
    "            break\n",
    "        i = max(0, j - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def corpus_of(path: str) -> str:\n",
    "    p = Path(path)\n",
    "    if 'delhi-food' in p.parts:\n",
    "        return 'delhi-food'\n",
    "    if 'poetry' in p.parts:\n",
    "        return 'poetry'\n",
    "    return 'unknown'\n",
    "\n",
    "all_chunks: List[Chunk] = []\n",
    "for doc in all_docs:\n",
    "    parts = simple_char_chunk(doc['text'], size=1800, overlap=300)\n",
    "    c = corpus_of(doc['source_path'])\n",
    "    for idx, part in enumerate(parts):\n",
    "        all_chunks.append(\n",
    "            Chunk(\n",
    "                chunk_id=f\"{Path(doc['source_name']).stem}::chunk_{idx}\",\n",
    "                source_name=doc['source_name'],\n",
    "                source_path=doc['source_path'],\n",
    "                corpus=c,\n",
    "                start_char=part['start'],\n",
    "                end_char=part['end'],\n",
    "                text=part['text'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "print('Total chunks:', len(all_chunks))\n",
    "print('Delhi chunks:', sum(1 for c in all_chunks if c.corpus == 'delhi-food'))\n",
    "print('Poetry chunks:', sum(1 for c in all_chunks if c.corpus == 'poetry'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save chunk artifacts for LangChain/LlamaIndex ingestion\n",
    "\n",
    "chunks_jsonl = CHUNKS_DIR / 'all_chunks.jsonl'\n",
    "with chunks_jsonl.open('w', encoding='utf-8') as f:\n",
    "    for c in all_chunks:\n",
    "        f.write(json.dumps(asdict(c), ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Saved:', chunks_jsonl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36c580",
   "metadata": {},
   "source": [
    "## DSPy-Style Synthetic Dialogue Skeleton\n",
    "\n",
    "Paper-style usage:\n",
    "1. Generate a conversation-starting user query from a knowledge chunk.\n",
    "2. Generate a concise assistant answer + one follow-up question.\n",
    "\n",
    "Below is a minimal implementation skeleton using LiteLLM.\n",
    "You can later swap this with native DSPy signatures/modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "LLM_MODEL = os.getenv('SYNTHETIC_GEN_MODEL', 'gemini/gemini-2.0-flash')\n",
    "\n",
    "QUESTION_PROMPT = (\n",
    "    \"Generate one conversation-initiating statement in English/Hinglish based on the knowledge. \"\n",
    "    \"Use varied starts like why/when/where/how. Keep it natural and unique.\\n\\n\"\n",
    "    \"KNOWLEDGE:\\n{knowledge}\"\n",
    ")\n",
    "\n",
    "ANSWER_PROMPT = (\n",
    "    \"Give an informative response in English in 2 lines, grounded in the knowledge. \"\n",
    "    \"Then ask one thoughtful follow-up question in English/Hinglish. \"\n",
    "    \"Do not generate more turns.\\n\\n\"\n",
    "    \"KNOWLEDGE:\\n{knowledge}\\n\\n\"\n",
    "    \"USER_QUESTION:\\n{question}\"\n",
    ")\n",
    "\n",
    "\n",
    "def llm_call(prompt: str, model: str = LLM_MODEL, temperature: float = 0.4) -> str:\n",
    "    resp = completion(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def generate_synthetic_pair(knowledge_text: str) -> Dict[str, str]:\n",
    "    q = llm_call(QUESTION_PROMPT.format(knowledge=knowledge_text[:4000]), temperature=0.7)\n",
    "    a = llm_call(ANSWER_PROMPT.format(knowledge=knowledge_text[:4000], question=q), temperature=0.4)\n",
    "    return {'question': q, 'answer': a}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run on a small sample (set RUN_SYNTHETIC=True when ready)\n",
    "\n",
    "RUN_SYNTHETIC = False\n",
    "SAMPLE_N = 8\n",
    "\n",
    "synthetic_rows = []\n",
    "if RUN_SYNTHETIC:\n",
    "    sample_chunks = [c for c in all_chunks if c.corpus == 'delhi-food'][:SAMPLE_N]\n",
    "    for c in sample_chunks:\n",
    "        pair = generate_synthetic_pair(c.text)\n",
    "        synthetic_rows.append({\n",
    "            'chunk_id': c.chunk_id,\n",
    "            'source_name': c.source_name,\n",
    "            'corpus': c.corpus,\n",
    "            **pair,\n",
    "        })\n",
    "\n",
    "print('Synthetic rows:', len(synthetic_rows))\n",
    "if synthetic_rows:\n",
    "    print(synthetic_rows[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training-ready JSONL skeleton\n",
    "\n",
    "out_jsonl = EXPORT_DIR / 'synthetic_dialogues.jsonl'\n",
    "if synthetic_rows:\n",
    "    with out_jsonl.open('w', encoding='utf-8') as f:\n",
    "        for row in synthetic_rows:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + '\\n')\n",
    "    print('Saved:', out_jsonl)\n",
    "else:\n",
    "    print('No rows exported. Set RUN_SYNTHETIC=True and rerun generation cell.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e480d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Replace `simple_char_chunk` with heading-aware/semantic chunking.\n",
    "- Add OCR fallback for scanned PDFs.\n",
    "- Add deduplication + quality filters for generated dialogues.\n",
    "- Add separate generation configs for `delhi-food` vs `poetry` styles.\n",
    "- Add train/val/test split exports.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
